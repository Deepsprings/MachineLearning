# 机器学习之集成学习（Ensemble Learning）

> **前言： ** 学习笔记，记录下对于一些问题的记录和理解，复习和加深记忆用，挖坑补坑用。
>
> 参考：*李航 《统计学习方法》*

## 0. 集成学习

* 集成学习思想是组合多个弱分类器（基分类器），从而得到一个更好更全面的强分类模型。即，将几种机器学习技术组合，达到减少偏差、方差或改进预测的效果。
* 基分类器可以使用**不同类别**的基本分类器(ID3/C4.5/CART/SVM/LOGISTIC等)，组合。
* 模型组合方式：线性组合，乘积组合，投票组合
* 串行（序列）学习：通常对所有样本进行训练，各个学习器间有相互依赖关系，会根据学习器结果分配权重，然后给后面的分类器继续训练。
* 并行学习：各基础分类器具有独立性，通常使用多数表决方式决定最后结果。
* 对于大数据集：可以划分多个小数据集，学习多个模型，组合。
* 对于小数据集：可以从总体样本中抽样，得到多个数据集进行训练，组合。
* 集成学习的两大类别
  * Boosting(串行)
    * AdaBoost
    * GBDT
    * XGBoost
    * lightGBM
  * Bagging (bootstrap aggregating ; 并行)
    * RF
* Boosting与Bagging区别
  * 对于Boosting：训练集是固定不变，每个样例的权重根据上一轮的分类结果而改变，各个分类器间有联系，顺序生成
  * 对于Bagging：训练集由原始训练集有放回抽样选取，均匀取样，因此每个样例的权重相同，各个分类器间相互独立，并行生成

## 1. Boosting

* 主要思想是将弱分类器组装成强分类器。依据是，在PAC（概率近似正确）学习框架(**what?**)下，一定可以将弱分类器组装成强分类器。

* 其关注点在于**降低偏差**，**防止欠拟合**

* 主要包含着两个核心问题
  1.  如果改变训练数据的权值或概率分布  
  2.  如何组合弱分类器

* AdaBoost
  * 算法过程
    * 初始化训练数据的权值分布
    * 使用具有权值分布的训练数据集合学习，得到基本分类器
    * 计算分类误差率$e_m$
    * 根据分类误差率计算基本分类器的权重$\alpha_m$
    * 根据每个数据点的预测正确与否情况，更新数据集的权值分布$w_{m,i}$
    * 重复以上步骤，将构建的基本分类器进行**线性组合**
  * 特点
    * 应用于二分类学习
    
    * 训练数据不改变，通过改变数据权值分布，改变训练数据在不同基分类器中所起到的作用
    
    * 通过分类误差率和样本的分类正确与否，调整每个样本的权重。
    
    * 采用线性组合的方法，加权表决，得到最终的分类器。
    
    * 其存在着训练误差界限，训练误差是以**指数速率下降**的(**why?**)
    
    * 另一种解释：**加法模型 + 指数函数损失 + 前向分布算法**
      $$
      加法模型：f(x) = \sum_{m=1}^M\beta_mb(x;\theta_m) \\
      损失函数：L(y,f(x)) = \sum_{i=1}^NL(y_i,f_{m-1}(x)+\beta b(x_i;\theta)) \\
      极小化：(\beta_m,\theta_m) = argminL(y,f(x)) \\
      更新传递：f_m(x) = f_{m-1}(x)+\beta_m b(x;\theta_m)
      $$
    
  * 算法实现
  
* GBDT（Gradient Boosting Decision Tree）梯度提升决策树

  * 决策树(Decision Tree)

    * GBDT使用**CART回归树**：无论处理回归还是分类问题，因为每次拟合的是梯度值，为连续值。
    * 使用**平方误差**解决连续数值的回归问题

  * **提升树（Boosting Tree）**

    * 以决策树作为基分类器的提升方法，构造加法模型，可用于分类和回归

    * 对于回归问题使用平方误差损失，对于分类问题使用指数损失，对于一般决策使用一般损失函数。

    * 对分类问题就是Adaboost使用决策树作为基学习器的情况。

    * 对于回归问题，提升树使用前向分步算法

      1. 初始化

      $$
      f_0(x) = 0 \\
      $$

      2. 迭代
         $$
         f_{m-1}(x)->f_m(x):  f_m(x) = f_{m-1}(x)+T(x;\theta_m), m=1,2,...,M \\
         其中：\hat{\theta_m} = arg \mathop{min}\limits_{\theta_m}\sum\limits_{i=1}^{N}L(y_i,f_{m-1}(x_i)+T(x_i;\theta_m))\\
         (平方误差)损失函数：L(y,f_{m-1}(x)+T(x;\theta_m)) = (y-f_{m-1}(x)-T(x;\theta_m))^2 = (r-T(x;\theta_m))^2 \\
         其中：r = y-f_{m-1}(x)
         也即是模型拟合数据的残差
         $$

      3. 结果
         $$
         f_M(x) = \sum_{m=1}^MT(x,\theta_m)
         $$

    * 算法

      1. 初始化$f_0(x) = 0$

      2. 对于m=1,2,3,...,M

         * 计算残差
           $$
           r_m = y_i - f_{m-1}(x), i=1,2,...,N
           $$

         * 拟合残差：$T(x;\theta_m)$

         * 更新：$f_m(x) = f_{m-1}(x)+T(x;\theta_m)$

      3. 得到回归提升树：$f_M(x) = \sum_{m=1}^MT(x,\theta_m) $

  * 梯度提升（Grandient Boosting）

    * 残差利用平方误差损失，不好计算，于是使用最速下降法，寻找能代替残差的值：**负梯度**
      $$
      -[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{t-1}(x)}\\
      对于平方损失函数，L(y,f(x_i)) = \frac{1}{2}(y-f(x_i))^2\\
      负梯度为：y-f(x_i)也就是上述残差
      $$

  * 梯度提升树（GBDT）

    * 算法

      1. 初始化弱学习器
         $$
         f_0(x) = arg \mathop{min}\limits_c \sum\limits_{i=1}^NL(y_i,c)
         $$

      2. 对于m=1,2,...,M

         * 对每个样本，计算负梯度：i=1,2,...,N，
           $$
           r_{mi} = -[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{t-1}}
           $$

         * 拟合$r_{mi}$，得到回归树

         * 对j=1,2,...,J，
           $$
           c_{mj} = arg \mathop{min}\limits_c \sum\limits_{x_i in R_{mj}}L(y_i,f_{m-1}(x_i)+c)
           $$

         * 更新
           $$
           f_m(x) = f_{m-1}(x)+\sum\limits_{j=1}^Jc_{mj}I(x\in R_{mj})
           $$

      3. 最终回归树
         $$
         f(x) = f_M(x) =\sum\limits_{m=1}^M\sum\limits_{j=1}^Jc_{mj}I(x\in R_{mj})
         $$

    * 实现

* xGBoosting（eXtreme Gradient Boosting Decision Tree）极致梯度提升决策树
  
  * 基本内容
  
* lightGBM

## 2. Bagging 

* 采用自助采样法（Bootstrap sampling），从总体样本中挑选出T个含M个训练样本的数据集，分别对每个数据集训练出一个学习器，然后进行**模型融合**，**对于回归问题使用简单平均法，对于分类问题，使用投票表决法**得到最终结果。
* 其关注点在于**降低方差**，**防止过拟合**，可以用于**多分类和回归**问题中。

* Bootstrap方法

  * 从总体样本数据(N)中**有放回**的**随机**选取M个样本。因此每个样本不被抽中的概率为$p = (\frac{N-1}{N})^M = (1-\frac{1}{N})^M$，当N和M都非常大时，p = 36.8%

* RF随机森林

  * 随机森林采用以**决策树**为基学习器构建Bagging集成学习。可若选用同种决策树，即按照相同规则（如基尼指数）决定每个节点的话，那么每棵树都是相同的，因此随机森林中，添加了一个**随机性**，即：在选取决策树的属性时，先从该节点属性结合中随机选择k个属性，然后从这k个属性中选择一个最优属性用于划分。这样就保证了基决策树的多样性。

  * 两个随机性：样本随机，每个树的节点属性选择也带有一定随机性。因此最终分类器可以具有较好的泛华性能。

  * 优点

    1. 可以处理分类和回归问题
    2. 能够处理高维数据，不需要降维：特征子集的大小可以控制
    3. 训练速度快：树与树之间相互独立，可以采用并行训练
    4. 对于异常点不敏感，有部分特征遗失，仍可以维持准确度：其随机性以及投票抉择的必然结果
    5. 可以评估各个特征在分类上的重要性

  * 错误率

    * 树的相关性越大，错误率增加
    * 每棵树的分类能力越大，错误率减小
    * **特征个数M的选择**：与上述两者成正相关。所以需要权衡选择特征个数M
    * **oob error**（袋外错误率)：如上述，在每棵树的生成训练中，会有大约1/3的实例（obb 样本）没有参与，因此可以将每棵树的obb样本当做验证集，计算错误率。这种方法是属于在内部就进行了评估，而非外部交叉验证。

  * 调参(**how?**)

    ```python
    classsklearn.ensemble.RandomForestClassifier(
            n_estimators=10, criterion='gini',
            max_depth=None,min_samples_split=2, 
            min_samples_leaf=1, min_weight_fraction_leaf=0.0,
            max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07,bootstrap=True,
            oob_score=False, n_jobs=1, 
            random_state=None, verbose=0,
            warm_start=False, class_weight=None)
    
    ```



## 3. 关于模型融合

通俗地讲，模型融合就是综合考虑不同的模型情况，将他们的结果通过一定策略融合到一起。

* 简单方法
  * 投票
  * 加权
  * 取平均
* 复杂点
  * blending
  * stacking

## 4. 关于blending与stacking

* 它们基本上是串并行方法组合，融合出一个模型，灵活性很高，可以组合出一个很复杂的模型

* 简单实例：先使用并行方法，训练出多个独立的学习器，然后根据这些学习器的输出值，在串接训练器（次学习器），得到上述独立训练器组合策略（RF中直接使用投票/取平均的方法），从而得到一个最终输出。实际应用中，常常串接一个 logistic 训练器

* blending

  * 示意图

  ![](/home/zhangwei/Screenshot from 2020-04-16 15-17-32.png)

  * 流程
    1. **划分训练集**：将训练数据集按比例划分为训练集、验证集
    2. **训练第一层**：创建第一层模型（可异质），用训练集进行训练
    3. 使用验证集预测得到预测值
    4. **训练第二层**：将上面得到的预测值当做第二层模型的输入，训练第二层模型
    5. **模型泛化测试**：同样步骤，使用测试集合先在第一层模型得到预测值，在输入到第二层模型，得到最终值

* stacking

  * 示意图

  ![](/home/zhangwei/Screenshot from 2020-04-16 15-17-12.png)

  * 流程
    1. **划分数据集**如上图，将训练集划分为四等分，如分别记为Fold1, Fold2, Fold3, Fold4。
    2. **训练第一层**：对于第一个训练模型，将Fold2, Fold3, Fold4作为上述的训练集，Fold1作为上述的验证集；对于第二个训练模型，将Fold2作为验证集，其他作为训练集；以此类推...
    3. 后面步骤同blending类似

* 两者比较

  |                |          blending          |        stacking        |
  | :------------: | :------------------------: | :--------------------: |
  | 数据集划分方法 |         HoudOut CV         |       K-Fold CV        |
  |      优点      |   实现简单；避免信息泄露   |   效果很好；更加稳健   |
  |      缺点      | 数据利用量少；过拟合概率高 | 更复杂；可能会信息泄露 |