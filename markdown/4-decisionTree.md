# 机器学习之决策树(decision tree)

> **前言： ** 学习笔记，记录下对于一些问题的记录和理解，复习和加深记忆用，挖坑补坑用。
>
> 参考：*李航 《统计学习方法》*

## 0. 基本内容

决策树可认为是定义在特征空间与类空间上的条件分布。其学习通常包括三个步骤：特征选择、决策树生成和决策树的剪枝。

## 1.导读

* 三要素：模型（决策函数），策略（损失函数和风险函数），算法（具体计算方法）

* 熵、条件熵、信息增益、信息增益率
  * 熵的由来，为什么能够度量信息
* 决策树的过程：特征选择，树的生成，剪枝
* ID3算法、实现、优缺点
  * 为什么不能处理连续值，缺失值
* C4.5
* CART
  * 为什么用基尼指数代替熵
  * 树的剪枝
* 随机森林
* GBDT
  * DT树
  * GB梯度迭代
  * 工作原理和过程
* Adaboosting

## 2. 问题与理解

* 熵，信息熵，联合熵，条件熵

  * 公式
    $$
    熵：H(X) = - \sum_{i=1}^np(x_i)logp(x_i) \\
    联合熵：H(X,Y) = -\sum_{i=1}^np(x_i,y_i)logp(x_i,y_i)   \\
    条件熵：H(Y|X) = -\sum_{i=1}^np(x_i,y_i)logp(x_i|y_i) = \sum_{j=1}^np(y_j)H(X|y_j)
    $$
    

  * 熵最早起源于热力学，表征能量分布的混乱程度。信息熵表征信息的混乱程度，或者说是**信息的不确定性**。

  * 熵越大，表征信息的不确定性越大，或者说对于事件来讲，事件的信息量知道的越少。

  * 条件熵，表示在知道X信息的条件下，Y剩下的不确定性。

* 信息增益，信息增益比

  * 公式
    $$
    信息增益：g(D,A) = H(D) - H(D|A) \\
    信息增益比：g_R(D,A) = \frac{g(D,A)}{H_A(D)} \\
    其中：H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}
    $$

  * 两者之差又称作两者的互信息。决策树中的信息增益等价于训练数据集中类与特征的互信息

  * 信息增益顾名思义，就是信息量的增加值，获取到了多少的信息量（确定性）

  * 信息增益会偏向于取值较多的特征；信息增益比就是在信息增益的基础上增加了一个罚项，当种类越多时，惩罚程度增加

  * 信息增益的计算

  * 信息增益比的计算

* ID3算法

  * 使用**信息增益**最大的特征递归地构建当前节点
  * 终止条件：1. 特征的信息增益小于给定阈值（选择最多的类别作为当前节点）2. 没有特征可供选择 &3. 到达指定深度 4. 节点特征的数据量小于一定值
  * 该算法相当于用**极大似然估计**进行概率模型的选择，为局部学习策略
    * **why?**
  * 缺点（没有考虑不代表不能用，只是该算法没有考虑到而已）
    * 采用信息增益作为特征选择标准，容易偏向与取值较多的特征
    * 没有考虑剪枝，结构可能过于复杂，容易过拟合（决策树算法的通病）
    * 没有考虑连续性的特征
    * 没有考虑缺失值的问题
    * 没有考虑过拟合的问题
    * 只能用于分类
  * 算法实现

* C4.5算法

  * 使用**信息增益比**作为选择特征的标准

  * 终止条件：同上

  * 对ID3算法的改进

    * 使用信息增益比

    * 处理连续值：将连续特征离散化

      1. 先排序分段

      2. 取两样本均值，作为划分点

      3. 在每个划分点计算其**信息增益**（对于分叉划分更准确），选择最大信息增益点作为最终划分点（二叉），也就转化为了离散特征

      4. 计算**信息增益比**（对于特征选择更准确 ）选择特征

    * 处理缺失值：

      * 选择划分属性时：
        1. 将样本数据分为有该特征和无该特征两部分，每部分设置一个权重（**权重如何设置？**）
        2. 计算有该特征的信息增益比
        3. 最终的信息增益比取上述增益比乘以一个系数（加权后的无缺失部分占全体的比例）

      * 对有特征缺失的属性划分时：
        1. 将有缺失部分划分到每个特征中
        2. 划分到每个特征的比例按无缺失部分比例计算

  * 缺点：

    * 容易过拟合，通过剪枝优化：预剪枝；后剪枝
    * 多叉数模型，计算中还包含对数项，且计算中需要多次扫描数据，运算耗时，效率较低
    * 需要将数据存放在内存，适合较小规模数据
    * 只能用于分类

  * 算法实现

* CART算法

  * 可用于分类和回归

  * 决策树是二叉树，在给定的X条件下输出Y的条件概率分布

  * 分类树：使用基尼指数最小化准则

    * 基尼指数（表征信息的不确定性，与熵类似，但计算起来比熵的效率高，而且每个节点还是二分类）
      $$
      Gini(p) = \sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2\\
      二分类：Gini(p) = 2p(1-p)
      $$

    * 基尼指数计算
    * 终止条件：同上
    * 算法实现

  * 回归树：使用平方误差最小化准则

    * **how?**
    * 算法实现

* 树的剪枝

  * 公式：
    $$
    损失函数：C_\alpha(T) = \sum_{i=1}^{|T|}N_tH_t(T) + \alpha|T| \\
    其中：H_t(T) = -\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
    $$
    

  * 决策树的剪枝是考虑全局学习的模型

  * 利用损失函数最小化原则等价于正则化的极大似然估计（**why?**）

  * CART剪枝（**how?**）

    1. 使用上述剪枝方法，设定连续的$\alpha$值，得到一系列的子树序列
    2. 通过交叉验证的方式，得到最优子树

* 其他相关

  * 决策回归树与逻辑回归的区别
  * 随机森林（RF）
  * 梯度下降树（GBDT）