# 机器学习之最大熵模型（Maximum Entropy Model）

> **前言：** 学习笔记，记录下对于一些问题的记录和理解，复习和加深记忆用，挖坑补坑用。
>
> 参考：*李航 《统计学习方法》*

## 0. 基本内容

* 最大熵原理

  * 熵：表征信息的不确定程度(**how?**)
    $$
    H(p) = - \sum\limits_x P(x) logP(x)
    $$

  * 原理：在**满足约束条件**（如何规定？how？）的模型集合中选取**熵最大**(**可行性?why?**)的模型。也即在满足已知信息的约束条件下，剩下的作等概率处理。

    >在完全无约束的状态下，均匀分布等价于熵最大
    >
    >给定均值和方差，熵最大的分布等价于正态分布（why?）

* 最大熵模型

  * 最大熵原理应用到分类问题上的体现

  * 满足约束条件的模型集合 C 中条件熵 H(p) 最大的模型
    $$
    C = \{P\in \Omega | E_P(f_i)=E_{\mathop{P}\limits^\sim}(f_i),\  i=1,2,...,n\} \\
    H(P) = - \sum\limits_{x,y}\mathop{P}\limits^\sim (x)P(y|x)logP(y|x)
    $$

* 模型学习（对偶函数极大化等价与模型的极大似然估计）

  * 最优化问题
    $$
    \mathop{max}\limits_{P \in C} H(P) \ \ \  or\ \ \ \mathop{min}\limits_{P \in C}-H(P)\\
    s.t \ \ \ E_P(f_i) = E_{\mathop{P}\limits^\sim}(f_i), \ i=1,2,...,n \\
    \sum\limits_yP(y|x)=1
    $$
    
  * 对偶问题转化

    * 拉格朗日函数

    $$
    L(P,w)=-H(P)+w_0(1-\sum\limits_yP(y|x)+\sum\limits_{i=1}^n w_i(E_{\mathop{P}\limits^\sim}(f_i)-E_P(f_i)) \\
    $$

    * 原始问题
      $$
      \mathop{min}\limits_{P \in C} \mathop{max}\limits_w L(P,w)
      $$

    * 对偶问题
      $$
      \mathop{max}\limits_w \mathop{mim}\limits_{P \in C} L(P,w)
      $$

## 1. 问题与理解

* 关于条件熵 

  在决策树学习中，计算信息增益时，已经牵扯到了条件熵，推导从略
  $$
  H(Y|X) = - \sum\limits_{x,y}p(x,y)log\ p(y|x)
  $$
  问题是，在最大熵模型中如何理解**使用的是最大化条件熵**。

  首先，作为一个分类模型，一个**生成模型**（不同于逻辑回归的一个点），要解决的问题就是，给定 x 的条件下，模型输出对应类别。也即获得一个条件分布 P(y|x)，这正是条件熵公式里的包含项

  其次，对于最大化条件熵的含义，就是表示，知道X的信息下，在满足限制条件的情况下，使得 y 的取值混乱度最大

  >举个不恰当的例子说明：
  >
  >用前面决策树中申请信贷成功与否举例，若 x 表示有无房子，y 表示可否信贷(用0,1表示)。现在已知有房子的情况下 ，问信贷成功与否。
  >
  >假设没有先前经验，即没有任何限制条件。那么最大化条件熵，就是在已知有房子的情况下，y 信贷成功或失败的概率五五开（熵最大）。即 P(1|有房子)=1/2，P(0|有房子)=1/2，这就是根据最大熵模型得到的问题解决。
  >
  >但实际上，肯定是有先验经验的，可从样本中获得限制条件。至少从直觉来说，有房子情况下信贷成功的概率肯定要大于1/2，毕竟在决策树中还是信息增益最大的一个特征。
  >
  >那么问题又来了，**限制条件如何规定？如何从样本中提取到限制信息？**

  最后值得说的一点是，我们目的是通过最大化上面公式 H(Y|X)，进而得到公式中的 p(y|x)。这也即我们最终所得的最大熵模型。**注：熵H(p)的自变量为P(x)，条件熵自变量看作为P(y|x)**，后面最优化问题求导时会用到。

* 关于特征函数(Feature function)
  $$
  f(x,y) = \left\{
  \begin{array}  \\
  1, &x与y满足某一事实 \\
  0, &否则
  \end{array}  
  \right.
  $$
  书中是用上式表示特征函数。其实在概率论和集合论中都分别有特征函数这一概念。

  * 概率论中，特征函数（Characteristic function）是对分布函数的一个变换，一个特征函数对应着一个概率分布，其定义为
    $$
    \varphi X(t) = E(e^{itX})
    $$
    对于一个随机变量的特征，有诸如，期望（一阶矩）、方差（二阶矩）、偏态（三阶矩）等，而对于特征函数对应着唯一的概率分布，可从其泰勒展开上理解。确切说，其泰勒展开项包含着随机变量的各阶矩，即包含着随机变量的所有特征。因此也就等价于随机变量的概率分布的另一表示。即
    $$
    特征函数相等 \Rightarrow 各阶矩相等 \Rightarrow 各个特征相等 \Rightarrow 分布相等
    $$
    显然，$ Feature function \neq Characteristic function$

  * 信息论中，特征函数（Indicator function, 指示函数）常用于满足于某一条件的计数统计，表示为
    $$
    I_A(x)=\left\{
    \begin{array}\\
    1,  &x \in A \\
    0,  &x \notin A
    \end{array}
    \right.
    $$
    但其一般叫做指示函数。

  * 这里的特征函数功能其实就类似于集合论中的指示函数，简单来说就是起到统计满足某一条件的样本个数。**那么为什么要引入它呢，而不直接使用计数表示？**

    其实，这是因为通过这种方式，可以将数据集数学化表示，更体现数学之美，更简洁方便。

    >举例说明：假如有一个数据集，如下
    >
    >|      |  1   |  2   |  3   |  4   |  5   |
    >| :--: | :--: | :--: | :--: | :--: | :--: |
    >|  x   |  a   |  a   |  a   |  b   |  c   |
    >|  x0  |  S   |  M   |  M   |  L   |  L   |
    >|  y   |  0   |  1   |  0   |  0   |  1   |
    >
  >如果，让计算 P(x=a, y=0)，通过直接计数，观察到第1,3列为满足条件的，是很容易可以得到数 P = 2/5。
    >
    >但实际中，我们常常是需要一个表达式，例如，假设第一列数据有个权值w1，第三列有权值w3，那么让计算包含权值后的概率P呢？
    >
    >可以用计数表示 P = w1 + w2，其实这般表示已经默认是进行一个一个地计算，特征函数就是这种功能，$ P = \sum\limits_{n=1}^{5}w_n p(x_n,y_n) f(x_n,y_n) $，数学化表示方便公式推导和计算。
  
* 关于约束条件

  * 个人理解

    直观感受为，对于一个样本而言，我们可以得到样本的条件概率分布
    $$
    \mathop{P}\limits^\sim (Y|X)
    $$
    注意，根据我们设定的特征函数，样本中有很多组特征标签<feature, label>，对于每一组特征标签我们都可以计算出一个$ \mathop{P}\limits^\sim (y|x) $。对于该值如何计算获得，我们可以借助公式
    $$
    \mathop{P}\limits^\sim(y|x)=\frac{\mathop{p}\limits^\sim(x,y)}{\mathop{p}\limits^\sim(x)}
    $$
    而假设我们通过最大熵模型已经训练出了一个模型 P(y|x)，而要满足
    $$
    \mathop{P}\limits^\sim(y|x)=\mathop{P}(y|x)
    $$
    这样理解会有陷入一个自己想不通的误区：因为如果要满足了这样的限制条件，P(y|x) 都假设出真实值了，还需要什么的最优化问题求解，还有什么后面对P的求导。直觉是特征函数的理解上有偏差。结合具体实例编程来理解。

  * 书中推导步骤很详细
  $$
    P_w(y|x) = \frac{1}{Z_x(x)}exp(\sum\limits_{i=1}^n w_i f_i(x,y))  \\
    其中: Z_w(x)=\sum\limits_y exp(\sum\limits_{i=1}^n w_i f_i(x,y))
  $$
  
* 其他
  
  * 形式上跟提升模型有相似之处
  * 最大熵模型属于生成模型，逻辑回归属于判别模型；两者都为概率模型，都为对数线性模型。
